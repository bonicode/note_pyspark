{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark Note.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNiYHvINUxhs/S1gsg0WzDd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bonioloff/note_pyspark/blob/main/PySpark_Note.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation on how Spark works:\n",
        "- https://stackoverflow.com/questions/32621990/what-are-workers-executors-cores-in-spark-standalone-cluster"
      ],
      "metadata": {
        "id": "Dzs68uSOlSTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the pyspark package\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "fUlx7fwBq6ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext"
      ],
      "metadata": {
        "id": "AjzjvFw_q_Qy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jINbbaLZqyyH"
      },
      "outputs": [],
      "source": [
        "# Initialize spark context\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"Spark App\")\n",
        "sc = SparkContext(conf=conf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with Resilient Distributed Dataset (RDD) Interface"
      ],
      "metadata": {
        "id": "zWRlZpNW5yPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RDD with SparkContext\n",
        "rdd = sc.textFile(\"sample_data/california_housing_test.csv\")"
      ],
      "metadata": {
        "id": "chJADF0ZrNmx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the first 5 rows\n",
        "rdd.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u6w2BA-up6p",
        "outputId": "7487ce33-a5b5-4646-be67-94b36085c971"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"',\n",
              " '-122.050000,37.370000,27.000000,3885.000000,661.000000,1537.000000,606.000000,6.608500,344700.000000',\n",
              " '-118.300000,34.260000,43.000000,1510.000000,310.000000,809.000000,277.000000,3.599000,176500.000000',\n",
              " '-117.810000,33.780000,27.000000,3589.000000,507.000000,1484.000000,495.000000,5.793400,270500.000000',\n",
              " '-118.360000,33.820000,28.000000,67.000000,15.000000,49.000000,11.000000,6.135900,330000.000000']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map operation to take median_income and median_house_value\n",
        "income_vs_homevalue = rdd.map(lambda x: x.split(\",\")).map(lambda x: (x[-2], x[-1]))\n",
        "income_vs_homevalue.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVuuZ0LAr0-s",
        "outputId": "74b63713-d873-49a4-c29a-4314b087f0f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('\"median_income\"', '\"median_house_value\"'),\n",
              " ('6.608500', '344700.000000'),\n",
              " ('3.599000', '176500.000000'),\n",
              " ('5.793400', '270500.000000'),\n",
              " ('6.135900', '330000.000000')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with SparkSQL, DataFrame (DF) and Datasets"
      ],
      "metadata": {
        "id": "_5INzzJc54WQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike RDD, DataFrame has some benefits:\n",
        "- "
      ],
      "metadata": {
        "id": "E0Bpcxye6Cob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "Oio7MqnjujNY"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}